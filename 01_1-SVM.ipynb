{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install IPython\n",
    "from IPython.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/KU-DIC/LG_time_series_day15.git #코랩 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [시계열 이상치 탐지 3 실습]\n",
    "# Gaussian Density Estimation & 1-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)\n",
    "- 함수 ( ) 안에서 shift+tab: arguments description. shift+tab+tab은 길게 볼 수 있도록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>__1. Data: NASA Bearing Dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 description <br>\n",
    "    - NASA Bearing Dataset은 NSF I/UCR Center의 Intelligent Maintenance System의 4개의 bearing에서 고장이 발생할 때까지 10분 단위로 수집된 센서 데이터이다. 본 데이터셋은 특정 구간에서 기록된 1-second vibration signal snapshots을 나타내는 여러 개의 파일로 구성되어 있다. 각 파일은 20 kHz 단위로 샘플링 된 20,480개의 data point를 포함하고 있으며, 각 파일의 이름은 데이터가 수집된 시간을 의미한다. 해당 데이터셋은 크게 3개의 데이터를 포함하고 있으며, 본 실습에서 사용하는 데이터는 bearing 1에서 outer race failure가 발생할 때까지 수집된 센서 데이터이다. <br><br>\n",
    "- 변수 설명\n",
    "    - 독립 변수 (4개): Bearing1, Bearing2, Bearing3, Bearing4 <br><br>\n",
    "- 출처: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_time_series_day15/image/intro1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv('/content/LG_time_series_day15/data/nasa_bearing_dataset.csv', index_col=0)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 기간의 데이터 분포 확인\n",
    "data.plot(figsize = (12, 6))\n",
    "plt.axvline(data.index[int(len(data) * 0.5)], c='black')\n",
    "plt.axvline(data.index[int(len(data) * 0.7)], c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. 데이터 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data[data['data_type'] == 'train'].iloc[:, :4]\n",
    "y_train = data[data['data_type'] == 'train'].iloc[:, -2].values\n",
    "\n",
    "X_test = data[data['data_type'] == 'test'].iloc[:, :4]\n",
    "y_test = data[data['data_type'] == 'test'].iloc[:, -2].values\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 기반으로 train/test 데이터에 대하여 standard scaling 적용 (평균 0, 분산 1) \n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), \n",
    "                              columns=X_train.columns, \n",
    "                              index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                             columns=X_test.columns, \n",
    "                             index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __2-1. Gaussian Density Estimation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Density Estimation\n",
    ">모든 데이터가 하나의 가우시안(정규) 분포로부터 생성됨을 가정 <br>\n",
    ">학습: 주어진 정상 데이터들을 통해 가우시안 분포의 평균 벡터와 공분산 행렬을 추정 <br>\n",
    ">테스트: 새로운 데이터에 대하여 생성 확률을 구하고 이 확률이 낮을수록 이상치에 가까운 것으로 판정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Train 데이터 기반 분포 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가우시안 분포 추정 함수\n",
    "def estimate_gaussian(dataset):\n",
    "    mu = np.mean(dataset, axis=0).values\n",
    "    sigma = np.cov(dataset.T)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = estimate_gaussian(X_train)\n",
    "\n",
    "print('Dimension of mu:', mu.shape)\n",
    "print('Dimension of sigma:', sigma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 추정된 분포를 기반으로 train/test 데이터의 anomaly score 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변량 가우시안 분포에서 -logpdf를 반환하는 함수\n",
    "def multivariate_gaussian(dataset, mu, sigma):\n",
    "    p = multivariate_normal(mean=mu, cov=sigma)\n",
    "    return - 1.0 * p.logpdf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터의 -logpdf 값 도출\n",
    "gauss_train = multivariate_gaussian(X_train, mu, sigma)\n",
    "gauss_test = multivariate_gaussian(X_test, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터의 anomaly score 분포 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize = (8, 8))\n",
    "\n",
    "sns.distplot(gauss_train, bins = 100, kde= True, color = 'blue', ax=ax1)\n",
    "sns.distplot(gauss_test, bins = 100, kde= True, color = 'red', ax=ax2)\n",
    "ax1.set_title(\"Train Data\")\n",
    "ax2.set_title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Threshold 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold 탐색\n",
    "# score의 min ~ max 범위를 num_step개로 균등 분할한 threshold에 대하여 best threshold 탐색 \n",
    "def search_best_threshold(score, y_true, num_step):\n",
    "    best_f1 = 0.5\n",
    "    best_threshold = None\n",
    "    for threshold in np.linspace(min(score), max(score), num_step):\n",
    "        y_pred = threshold < score\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print('Best threshold: ', round(best_threshold, 4))\n",
    "    print('Best F1 Score:', round(best_f1, 4))\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best threshold 도출\n",
    "gauss_best_threshold = search_best_threshold(gauss_test, y_test, num_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Best threshold를 기반으로 이상치 탐지 모형 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 도출\n",
    "gauss_scores = pd.DataFrame(index=data.index)\n",
    "gauss_scores['score'] = list(np.hstack([gauss_train, gauss_test]))\n",
    "gauss_scores['anomaly'] = gauss_best_threshold < gauss_scores['score']\n",
    "gauss_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly score plot 도출\n",
    "def draw_plot(scores, threshold):\n",
    "    normal_scores = scores[scores['anomaly'] == False]\n",
    "    abnormal_scores = scores[scores['anomaly'] == True]\n",
    "\n",
    "    plt.figure(figsize = (12,5))\n",
    "    plt.scatter(normal_scores.index, normal_scores['score'], label='Normal', c='blue', s=3)\n",
    "    plt.scatter(abnormal_scores.index, abnormal_scores['score'], label='Abnormal', c='red', s=3)\n",
    "    \n",
    "    plt.axhline(threshold, c='green', alpha=0.7)\n",
    "    plt.axvline(data.index[int(len(data) * 0.5)], c='orange', ls='--')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터의 anomaly score 확인\n",
    "draw_plot(gauss_scores, gauss_best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FRR, FAR, F1 score 도출\n",
    "def calculate_metric(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[True, False])\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    \n",
    "    frr = fp / (fp + tn)\n",
    "    far = fn / (fn + tp) \n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return frr, far, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score: 0.9250\n",
    "frr, far, f1 = calculate_metric(y_test, gauss_scores['anomaly'].iloc[int(len(data) * 0.5):])\n",
    "\n",
    "print(\"**  FRR: {}  |  FAR: {}  |  F1 Score: {}\".format(round(frr, 4), round(far, 4), round(f1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __<br> 2-2. One Class Support Support Vector Machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class Support Vector Machine (OC-SVM)\n",
    ">영역 기반 학습 방법 <br>\n",
    ">학습: 정상적인 클래스의 데이터를 하나의 클래스에 속하도록 원점 좌표를 기준으로 기준선을 그어 Classification 진행 <br>\n",
    ">테스트: 새로운 데이터에 대하여 학습된 기준선을 넘으면 이상치에 가까운 것으로 판정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_time_series_day15/image/intro2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Train 데이터 기반 모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OC-SVM 설명\n",
    "    - kernl: kernel 유형. (‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed') 디폴트 'rbf'\n",
    "    - gamma: 'rbf', 'poly' 및 'sigmoid'에 대한 커널 계수\n",
    "    - nu: 훈련 오류 비율의 상한값과 지원 벡터 비율의 하한값, 디폴트 0.5\n",
    "    - max_iter: Solver 내의 iteration의 hard limit, no limit -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OC-SVM 모델 적합\n",
    "OCSVM_model = OneClassSVM(kernel = 'rbf', gamma = 0.0001, nu = 0.009, max_iter = -1)\n",
    "OCSVM_model.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 적합된 모델을 기반으로 train/test 데이터의 anomaly score 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터의 OCSVM score 도출\n",
    "OCSVM_train = -1 * OCSVM_model.score_samples(X_train_scaled)\n",
    "OCSVM_test = -1 * OCSVM_model.score_samples(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터의 anomaly score 분포 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize = (8, 8))\n",
    "\n",
    "sns.distplot(OCSVM_train, bins=100, kde=True, color='blue', ax=ax1)\n",
    "sns.distplot(OCSVM_test, bins=100, kde=True, color='red', ax=ax2)\n",
    "ax1.set_title(\"Train Data\")\n",
    "ax2.set_title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Threshold 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold 탐색\n",
    "# score의 min ~ max 범위를 num_step개로 균등 분할한 threshold에 대하여 best threshold 탐색 \n",
    "def search_best_threshold(score, y_true, num_step):\n",
    "    best_f1 = 0.5\n",
    "    best_threshold = None\n",
    "    for threshold in np.linspace(min(score), max(score), num_step):\n",
    "        y_pred = threshold < score\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print('Best threshold: ', round(best_threshold, 4))\n",
    "    print('Best F1 Score:', round(best_f1, 4))\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best threshold 도출\n",
    "OCSVM_best_threshold = search_best_threshold(OCSVM_test, y_test, num_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Best threshold를 기반으로 이상치 탐지 모형 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 도출\n",
    "OCSVM_scores = pd.DataFrame(index=data.index)\n",
    "OCSVM_scores['score'] = list(np.hstack([OCSVM_train, OCSVM_test]))\n",
    "OCSVM_scores['anomaly'] = OCSVM_best_threshold < OCSVM_scores['score']\n",
    "OCSVM_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터의 anomaly score 확인\n",
    "draw_plot(OCSVM_scores, OCSVM_best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score: 0.9291\n",
    "frr, far, f1 = calculate_metric(y_test, OCSVM_scores['anomaly'].iloc[int(len(data) * 0.5):])\n",
    "\n",
    "print(\"**  FRR: {}  |  FAR: {}  |  F1 Score: {}\".format(round(frr, 4), round(far, 4), round(f1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttest",
   "language": "python",
   "name": "ttest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
